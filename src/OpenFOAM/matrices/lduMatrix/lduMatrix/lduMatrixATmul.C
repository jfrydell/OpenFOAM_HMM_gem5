/*---------------------------------------------------------------------------*\
  =========                 |
  \\      /  F ield         | OpenFOAM: The Open Source CFD Toolbox
   \\    /   O peration     |
    \\  /    A nd           | www.openfoam.com
     \\/     M anipulation  |
-------------------------------------------------------------------------------
    Copyright (C) 2011-2016 OpenFOAM Foundation
    Copyright (C) 2017-2019 OpenCFD Ltd.
-------------------------------------------------------------------------------
License
    This file is part of OpenFOAM.

    OpenFOAM is free software: you can redistribute it and/or modify it
    under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    OpenFOAM is distributed in the hope that it will be useful, but WITHOUT
    ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
    FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License
    for more details.
    
    You should have received a copy of the GNU General Public License
    along with OpenFOAM.  If not, see <http://www.gnu.org/licenses/>.

Description
    Multiply a given vector (second argument) by the matrix or its transpose
    and return the result in the first argument.

\*---------------------------------------------------------------------------*/


//#define USE_OPT_LDU

#include "lduMatrix.H"



#ifdef USE_ROCTX
#include <roctracer/roctx.h>
#endif

#ifdef USE_OMP
#include <omp.h>


  #if defined(WM_SP)
  #define _FP_TYPE_scalar float
  #define _FP_TYPE_solve_scalar float
  #elif defined(WM_SPDP)
  #define _FP_TYPE_scalar float
  #define _FP_TYPE_solve_scalar double
  #elif defined(WM_DP)
  #define _FP_TYPE_scalar double
  #define _FP_TYPE_solve_scalar double
  #endif



  #ifndef OMP_UNIFIED_MEMORY_REQUIRED
  #pragma omp requires unified_shared_memory
  #define OMP_UNIFIED_MEMORY_REQUIRED
  #endif
#endif

#ifndef TARGET_CUT_OFF
#define TARGET_CUT_OFF 10000
#endif

//#define USM_LDU_MAT

// * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * //

void Foam::lduMatrix::Amul
(
    solveScalarField& Apsi,
    const tmp<solveScalarField>& tpsi,
    const FieldField<Field, scalar>& interfaceBouCoeffs,
    const lduInterfaceFieldPtrsList& interfaces,
    const direction cmpt
) const
{
    #ifdef USE_ROCTX
    roctxRangePush("lduMatrix::Amul");
    #endif

    solveScalar* __restrict__ ApsiPtr = Apsi.begin();

    const solveScalarField& psi = tpsi();
    const solveScalar* const __restrict__ psiPtr = psi.begin();

    const scalar* const __restrict__ diagPtr = diag().begin();

    const label* const __restrict__ uPtr = lduAddr().upperAddr().begin();
    const label* const __restrict__ lPtr = lduAddr().lowerAddr().begin();
    const label* const __restrict__ losortPtr =  lduAddr().losortAddr().begin(); 

    const scalar* const __restrict__ upperPtr = upper().begin();
    const scalar* const __restrict__ lowerPtr = lower().begin();

    const label startRequest = Pstream::nRequests();

#ifdef SAVE_LDU
    static int file_counter=0;
    if (file_counter < 2){
    int rank=Pstream::myProcNo();
    char name[512];
    FILE *pFile;

    sprintf(name, "uPtr.%d.%d.dat.%d",(int) lduAddr().upperAddr().size(),rank,file_counter);
    pFile = fopen(name,"w");
    fwrite ( (void*) uPtr, sizeof(label), lduAddr().upperAddr().size(), pFile );
    fclose(pFile);

    sprintf(name, "lPtr.%d.%d.dat.%d",(int) lduAddr().lowerAddr().size(),rank,file_counter);
    pFile = fopen(name,"w");
    fwrite ( (void*) lPtr, sizeof(label), lduAddr().lowerAddr().size(), pFile );
    fclose(pFile);

    sprintf(name, "upperPtr.%d.%d.dat.%d",(int) upper().size(),rank,file_counter);
    pFile = fopen(name,"w");
    fwrite ( (void*) upperPtr, sizeof(scalar), upper().size(), pFile );
    fclose(pFile);


    sprintf(name, "lowerPtr.%d.%d.dat.%d",(int) lower().size(),rank,file_counter);
    pFile = fopen(name,"w");
    fwrite ( (void*) lowerPtr, sizeof(scalar), lower().size(), pFile );
    fclose(pFile);

    sprintf(name, "psiPtr.%d.%d.dat.%d",(int) psi.size(),rank,file_counter);
    pFile = fopen(name,"w");
    fwrite ( (void*) psiPtr, sizeof(solveScalar), psi.size(), pFile );
    fclose(pFile);
    }
#endif

    // Initialise the update of interfaced interfaces
    initMatrixInterfaces
    (
        true,
        interfaceBouCoeffs,
        interfaces,
        psi,
        Apsi,
        cmpt
    );


    //printf("LG:  in Amul  file = %s line = %d, ApsiPtr = %p\n",__FILE__,__LINE__, ApsiPtr );
    
    const label nCells = diag().size();
#if 1 //mi300A

    const label nFaces = upper().size();  


    #ifdef USE_OPT_LDU
      static label *repetition_count_ofsetts = NULL;

      static label counter=0;
      if (repetition_count_ofsetts == NULL){

        label *repetition_count = new (std::align_val_t(256)) label[nFaces];
        label face = 0;
        while (face < nFaces){
            const label l_val = lPtr[face] ;
            label local_counter=0;
            while (counter<nFaces){
                 if (l_val == lPtr[face+local_counter])
                    local_counter++;
                 else
                    break;
            }
            repetition_count[counter] = local_counter;
            face += local_counter;
            counter++;
       }
       repetition_count_ofsetts = new (std::align_val_t(256)) label[counter+1];
       repetition_count_ofsetts[0] = 0;
       for (label i=1; i <= counter; i++)
          repetition_count_ofsetts[i] = repetition_count_ofsetts[i-1]+repetition_count[i-1];

       delete[] repetition_count;
    }


    #endif








    #pragma omp target teams distribute parallel for if(nCells>TARGET_CUT_OFF)
    for (label cell=0; cell<nCells; cell++)
    {
          ApsiPtr[cell] = diagPtr[cell]*psiPtr[cell];
    }

    #ifdef SAVE_LDU
    if (file_counter < 2){
    int rank=Pstream::myProcNo();
    char name[512];
    FILE *pFile;

    sprintf(name, "ApsiPtr.%d.%d.dat.%d",(int) Apsi.size(),rank,file_counter);
    pFile = fopen(name,"w");
    fwrite ( (void*) ApsiPtr, sizeof(solveScalar), Apsi.size(), pFile );
    fclose(pFile);
    }
    #endif

    //double t1 = omp_get_wtime();
    #if 0
    #pragma omp target teams distribute parallel for thread_limit(64)  if(nCells>TARGET_CUT_OFF)  
    for (label face=0; face<nFaces; face++)
    {
	  const label l_val = lPtr[face] ;
          const label u_val = uPtr[face];

          #pragma omp atomic 
          ApsiPtr[u_val] += lowerPtr[face]*psiPtr[l_val];
          #pragma omp atomic 
          ApsiPtr[l_val] += upperPtr[face]*psiPtr[u_val];
    }
    #else


      #ifdef USE_OPT_LDU

         
         #pragma omp target teams distribute parallel for
         for (label i=0; i < counter; i++){
            solveScalar sum = Foam::Zero;
            const label face_start = repetition_count_ofsetts[i];
            const label face_stop = repetition_count_ofsetts[i+1];

            const label l_val = lPtr[face_start]; 
            const solveScalar psiPtr_l_val = psiPtr[l_val];

	    #pragma unroll 2
            for (label face = face_start; face < face_stop; ++face){
 
                const label u_val = uPtr[face];
                #pragma omp atomic 
                ApsiPtr[u_val] += lowerPtr[face]*psiPtr_l_val;
                sum += upperPtr[face]*psiPtr[u_val];
            }
            #pragma omp atomic
            ApsiPtr[l_val] += sum;
        }
        
      #if 0
        const label loop_length = lduAddr().losortAddr().size()-1;
	fprintf(stderr,"rank = %d: loop_length = %d counter = %d\n",Pstream::myProcNo(),loop_length, counter);

        fprintf(stderr,"rank = %d: ",Pstream::myProcNo());
        for (label ii = 0; ii < 20; ++ii) fprintf(stderr,"%d  ",lPtr[ii]);
	fprintf(stderr,"\n");


        //#pragma omp target teams distribute parallel for thread_limit(256)  if (loop_length > 5000)
        for (label i=0; i < 4/*loop_length*/; i++){
           solveScalar sum = Foam::Zero;
           const label face_start = losortPtr[i];
           const label face_stop =  losortPtr[i+1];
           fprintf(stderr,"A: rank = %d: i = %d, face_start = %d, face_stop = %d\n",Pstream::myProcNo(), i, face_start, face_stop);
           fprintf(stderr,"B: rank = %d: i = %d, face_start = %d, face_stop = %d\n",Pstream::myProcNo(), i, repetition_count_ofsetts[i], repetition_count_ofsetts[i+1]);

         //  const label l_val = lPtr[face_start];
       //    const solveScalar psiPtr_l_val = psiPtr[l_val];

	   //for (label face = face_start; face < face_stop; ++face){
           //     const label u_val = uPtr[face];
           //     #pragma omp atomic
           //     ApsiPtr[u_val] += lowerPtr[face]*psiPtr_l_val;
           //     sum += upperPtr[face]*psiPtr[u_val];
           // }
           // #pragma omp atomic
           // ApsiPtr[l_val] += sum;
	}

        for (label i=counter-4; i < counter; i++){
           const label face_start = losortPtr[i];
           const label face_stop =  losortPtr[i+1];
           fprintf(stderr,"A: rank = %d: i = %d, face_start = %d, face_stop = %d\n",Pstream::myProcNo(), i, face_start, face_stop);
           fprintf(stderr,"B: rank = %d: i = %d, face_start = %d, face_stop = %d\n",Pstream::myProcNo(), i, repetition_count_ofsetts[i], repetition_count_ofsetts[i+1]);
	}
      #endif

      #else 
        #pragma omp target teams distribute parallel for thread_limit(64) if(nCells>TARGET_CUT_OFF)
        for (label face=0; face<nFaces; face+=2)
        {

            const label nf = (nFaces-face) > 1 ? 2 : 1;
            #pragma unroll 2
            for ( label i = 0; i < nf; ++i){
              const label l_val = lPtr[face+i] ;
              const label u_val = uPtr[face+i];

              #pragma omp atomic
              ApsiPtr[u_val] += lowerPtr[face+i]*psiPtr[l_val];
              #pragma omp atomic
              ApsiPtr[l_val] += upperPtr[face+i]*psiPtr[u_val];
            }
        }
      #endif

    #endif 
    //double t2 = omp_get_wtime();
    //fprintf(stderr,"rank = %d:  nFaces = %d, ldu time = %g\n",Pstream::myProcNo(), nFaces, t2-t1);



    #ifdef SAVE_LDU
    if (file_counter < 2){
    int rank=Pstream::myProcNo();
    char name[512];
    FILE *pFile;

    sprintf(name, "ApsiPtr2.%d.%d.dat.%d",(int) Apsi.size(),rank,file_counter);
    pFile = fopen(name,"w");
    fwrite ( (void*) ApsiPtr, sizeof(solveScalar), Apsi.size(), pFile );
    fclose(pFile);

    file_counter++;
    }
    #endif



#else

      #ifdef USE_OMP
      label target_offload_limit = 20000;
      //_FP_TYPE_solve_scalar*  ApsiPtr_work_array = (_FP_TYPE_solve_scalar*) omp_target_alloc(sizeof(_FP_TYPE_solve_scalar)*nCells, omp_get_default_device() );
      solveScalar*  ApsiPtr_work_array; 
      if (nCells>target_offload_limit)
        ApsiPtr_work_array  = (solveScalar*) omp_target_alloc(sizeof(solveScalar)*nCells, omp_get_default_device() );
      else
        ApsiPtr_work_array  = (solveScalar*) omp_target_alloc(sizeof(solveScalar)*nCells, omp_get_initial_device() );
      #endif

    //printf("LG:  in Amul  file = %s line = %d\n",__FILE__,__LINE__ );

    

    #pragma omp target teams distribute parallel for if(nCells>target_offload_limit)
    for (label cell=0; cell<nCells; cell++)
    {

        //ApsiPtr[cell] = diagPtr[cell]*psiPtr[cell];
        #ifdef USE_OMP
           ApsiPtr_work_array[cell] = diagPtr[cell]*psiPtr[cell];
        #else
          ApsiPtr[cell] = diagPtr[cell]*psiPtr[cell];
        #endif
    }
    //printf("LG:  in Amul  file = %s line = %d\n",__FILE__,__LINE__ );

    const label nFaces = upper().size();    


      #pragma omp target teams distribute parallel for if(nCells>target_offload_limit)  
      for (label face=0; face<nFaces; face++)
      {
        #ifdef USE_OMP
          #pragma omp atomic hint(AMD_fast_fp_atomics)
          ApsiPtr_work_array[uPtr[face]] += lowerPtr[face]*psiPtr[lPtr[face]];
          #pragma omp atomic hint(AMD_fast_fp_atomics)
          ApsiPtr_work_array[lPtr[face]] += upperPtr[face]*psiPtr[uPtr[face]];
        #else 
          #pragma omp atomic 
          ApsiPtr[uPtr[face]] += lowerPtr[face]*psiPtr[lPtr[face]];
          #pragma omp atomic 
          ApsiPtr[lPtr[face]] += upperPtr[face]*psiPtr[uPtr[face]];
        #endif

      }


    #ifdef USE_OMP
    #pragma omp target teams distribute parallel for if(nCells>target_offload_limit)
    for (label cell=0; cell<nCells; cell++)
    {
        ApsiPtr[cell] = ApsiPtr_work_array[cell];
    }

    if (nCells>target_offload_limit)
       omp_target_free(ApsiPtr_work_array, omp_get_default_device() );
    else
       omp_target_free(ApsiPtr_work_array, omp_get_initial_device() );

    #endif
    
#endif


    //printf("LG:  in Amul  file = %s line = %d\n",__FILE__,__LINE__ );

    #ifdef USE_ROCTX
    roctxRangePush("lduMatrix::Amul:updateMatrixInterfaces");
    #endif
    // Update interface interfaces
    updateMatrixInterfaces
    (
        true,
        interfaceBouCoeffs,
        interfaces,
        psi,
        Apsi,
        cmpt,
        startRequest
    );
    #ifdef USE_ROCTX
    roctxRangePop();
    #endif

    tpsi.clear();

    #ifdef USE_ROCTX
    roctxRangePop();
    #endif
}


void Foam::lduMatrix::Tmul
(
    solveScalarField& Tpsi,
    const tmp<solveScalarField>& tpsi,
    const FieldField<Field, scalar>& interfaceIntCoeffs,
    const lduInterfaceFieldPtrsList& interfaces,
    const direction cmpt
) const
{
    #ifdef USE_ROCTX
    roctxRangePush("lduMatrix::Tmul");
    #endif
    solveScalar* __restrict__ TpsiPtr = Tpsi.begin();

    const solveScalarField& psi = tpsi();
    const solveScalar* const __restrict__ psiPtr = psi.begin();

    const scalar* const __restrict__ diagPtr = diag().begin();

    const label* const __restrict__ uPtr = lduAddr().upperAddr().begin();
    const label* const __restrict__ lPtr = lduAddr().lowerAddr().begin();

    const scalar* const __restrict__ lowerPtr = lower().begin();
    const scalar* const __restrict__ upperPtr = upper().begin();

    const label startRequest = Pstream::nRequests();

    // Initialise the update of interfaced interfaces
    initMatrixInterfaces
    (
        true,
        interfaceIntCoeffs,
        interfaces,
        psi,
        Tpsi,
        cmpt
    );

    const label nCells = diag().size();
    #pragma omp target teams distribute parallel for if(nCells>TARGET_CUT_OFF)
    for (label cell=0; cell<nCells; cell++)
    {
        TpsiPtr[cell] = diagPtr[cell]*psiPtr[cell];
    }

    const label nFaces = upper().size();
    #pragma omp target teams distribute parallel for if(nCells>TARGET_CUT_OFF) 
    for (label face=0; face<nFaces; face++)
    {
	const label l_val = lPtr[face];
        const label u_val = uPtr[face];	
        #pragma omp atomic   
        TpsiPtr[u_val] += upperPtr[face]*psiPtr[l_val];
	#pragma omp atomic
        TpsiPtr[l_val] += lowerPtr[face]*psiPtr[u_val];
    }

    // Update interface interfaces
    updateMatrixInterfaces
    (
        true,
        interfaceIntCoeffs,
        interfaces,
        psi,
        Tpsi,
        cmpt,
        startRequest
    );

    tpsi.clear();

    #ifdef USE_ROCTX
    roctxRangePop();
    #endif
}


void Foam::lduMatrix::sumA
(
    solveScalarField& sumA,
    const FieldField<Field, scalar>& interfaceBouCoeffs,
    const lduInterfaceFieldPtrsList& interfaces
) const
{
    #ifdef USE_ROCTX
    roctxRangePush("lduMatrix::sumA");
    #endif

    solveScalar* __restrict__ sumAPtr = sumA.begin();

    const scalar* __restrict__ diagPtr = diag().begin();

    const label* __restrict__ uPtr = lduAddr().upperAddr().begin();
    const label* __restrict__ lPtr = lduAddr().lowerAddr().begin();

    const scalar* __restrict__ lowerPtr = lower().begin();
    const scalar* __restrict__ upperPtr = upper().begin();

    const label nCells = diag().size();
    const label nFaces = upper().size();

    #pragma omp target teams distribute parallel for if(nCells>TARGET_CUT_OFF)
    for (label cell=0; cell<nCells; cell++)
    {
        sumAPtr[cell] = diagPtr[cell];
    }


    #pragma omp target teams distribute parallel for thread_limit(256) if(nFaces>TARGET_CUT_OFF)
    for (label face=0; face<nFaces; face+=2)
    {
	const label nf = (nFaces-face) > 1 ? 2 : 1;
        #pragma unroll 2
        for ( label i = 0; i < nf; ++i){
           #pragma omp atomic    
           sumAPtr[uPtr[face+i]] += lowerPtr[face+i];
	   #pragma omp atomic
           sumAPtr[lPtr[face+i]] += upperPtr[face+i];
	}
    }

    // Add the interface internal coefficients to diagonal
    // and the interface boundary coefficients to the sum-off-diagonal
    forAll(interfaces, patchi)
    {
        if (interfaces.set(patchi))
        {
            const labelUList& pa = lduAddr().patchAddr(patchi);
            const scalarField& pCoeffs = interfaceBouCoeffs[patchi];


            //forAll(pa, face)
	    const label loop_len = pa.size();
	    #pragma omp target teams distribute parallel for thread_limit(128) if(loop_len>3000)
	    for (label face=0; face<loop_len; face++)
            {
		#pragma omp atomic    
                sumAPtr[pa[face]] -= pCoeffs[face];
            }
        }
    }
    #ifdef USE_ROCTX
    roctxRangePop();
    #endif
}


void Foam::lduMatrix::residual
(
    solveScalarField& rA,
    const solveScalarField& psi,
    const scalarField& source,
    const FieldField<Field, scalar>& interfaceBouCoeffs,
    const lduInterfaceFieldPtrsList& interfaces,
    const direction cmpt
) const
{
    #ifdef USE_ROCTX
    roctxRangePush("lduMatrix::residual");
    #endif
    solveScalar* __restrict__ rAPtr = rA.begin();

    const solveScalar* const __restrict__ psiPtr = psi.begin();
    const scalar* const __restrict__ diagPtr = diag().begin();
    const scalar* const __restrict__ sourcePtr = source.begin();

    const label* const __restrict__ uPtr = lduAddr().upperAddr().begin();
    const label* const __restrict__ lPtr = lduAddr().lowerAddr().begin();

    const scalar* const __restrict__ upperPtr = upper().begin();
    const scalar* const __restrict__ lowerPtr = lower().begin();

    // Parallel boundary initialisation.
    // Note: there is a change of sign in the coupled
    // interface update.  The reason for this is that the
    // internal coefficients are all located at the l.h.s. of
    // the matrix whereas the "implicit" coefficients on the
    // coupled boundaries are all created as if the
    // coefficient contribution is of a source-kind (i.e. they
    // have a sign as if they are on the r.h.s. of the matrix.
    // To compensate for this, it is necessary to turn the
    // sign of the contribution.

    const label startRequest = Pstream::nRequests();

    // Initialise the update of interfaced interfaces
    initMatrixInterfaces
    (
        false,
        interfaceBouCoeffs,
        interfaces,
        psi,
        rA,
        cmpt
    );

    const label nCells = diag().size();
    #pragma omp target teams distribute parallel for if(nCells>TARGET_CUT_OFF)
    for (label cell=0; cell<nCells; cell++)
    {
        rAPtr[cell] = sourcePtr[cell] - diagPtr[cell]*psiPtr[cell];
    }


    const label nFaces = upper().size();
#if 0
    #pragma omp target teams distribute parallel for if(nFaces>TARGET_CUT_OFF)
    for (label face=0; face<nFaces; face++)
    {
        const label l_val = lPtr[face];
        const label u_val = uPtr[face];

        #pragma omp atomic    
        rAPtr[u_val] -= lowerPtr[face]*psiPtr[l_val];
         #pragma omp atomic
        rAPtr[l_val] -= upperPtr[face]*psiPtr[u_val];
    }

#else
    #pragma omp target teams distribute parallel for if(nFaces>TARGET_CUT_OFF)
    for (label face=0; face<nFaces; face+=2)
    {
	const label nf = (nFaces-face) > 1 ? 2 : 1;
        #pragma unroll 2
        for ( label i = 0; i < nf; ++i){
  	  const label l_val = lPtr[face+i];
          const label u_val = uPtr[face+i];
          #pragma omp atomic    
          rAPtr[u_val] -= lowerPtr[face+i]*psiPtr[l_val];
          #pragma omp atomic
          rAPtr[l_val] -= upperPtr[face+i]*psiPtr[u_val];
	}
    }
#endif

    // Update interface interfaces
    updateMatrixInterfaces
    (
        false,
        interfaceBouCoeffs,
        interfaces,
        psi,
        rA,
        cmpt,
        startRequest
    );
    #ifdef USE_ROCTX
    roctxRangePop();
    #endif
}


Foam::tmp<Foam::Field<Foam::solveScalar>> Foam::lduMatrix::residual
(
    const solveScalarField& psi,
    const scalarField& source,
    const FieldField<Field, scalar>& interfaceBouCoeffs,
    const lduInterfaceFieldPtrsList& interfaces,
    const direction cmpt
) const
{
    tmp<solveScalarField> trA(new solveScalarField(psi.size()));
    residual(trA.ref(), psi, source, interfaceBouCoeffs, interfaces, cmpt);
    return trA;
}


Foam::tmp<Foam::scalarField> Foam::lduMatrix::H1() const
{
    #ifdef USE_ROCTX
    roctxRangePush("lduMatrix::H1");
    #endif
    auto tH1 = tmp<scalarField>::New(lduAddr().size(), Zero);

    if (lowerPtr_ || upperPtr_)
    {
        scalar* __restrict__ H1Ptr = tH1.ref().begin();

        const label* __restrict__ uPtr = lduAddr().upperAddr().begin();
        const label* __restrict__ lPtr = lduAddr().lowerAddr().begin();

        const scalar* __restrict__ lowerPtr = lower().begin();
        const scalar* __restrict__ upperPtr = upper().begin();

        const label nFaces = upper().size();

	//double t1 = omp_get_wtime();
        #pragma omp target teams distribute parallel for thread_limit(64) if(nFaces>10000)
        for (label face=0; face<nFaces; face+=2)
        {
            const label nf = (nFaces-face) > 1 ? 2 : 1;
            #pragma unroll 2
            for ( label i = 0; i < nf; ++i){		
               #pragma omp atomic		
               H1Ptr[uPtr[face+i]] -= lowerPtr[face+i];
               #pragma omp atomic
               H1Ptr[lPtr[face+i]] -= upperPtr[face+i];
	    }
        }
        //double t2 = omp_get_wtime();
        //fprintf(stderr,"rank = %d: H1:  nFaces = %d, ldu time = %g\n",Pstream::myProcNo(), nFaces, t2-t1);

    }
    #ifdef USE_ROCTX
    roctxRangePop();
    #endif
    return tH1;
}


// ************************************************************************* //
